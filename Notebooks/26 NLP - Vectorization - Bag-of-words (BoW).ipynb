{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a51df6",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "\n",
    "Vectorization is the process of converting text data to numeric form, for processing by ML Models.\n",
    "\n",
    "# Bag-of-words (BoW)\n",
    "\n",
    "BoW is a text vectorization technique, which converts each unique token (word/characters etc.) or group of tokens into a feature (or column), with its value being represented by the token frequency (number of times it occurs in the document) or binary (if binary BoW is implemented)\n",
    "\n",
    "# About this notebook\n",
    "\n",
    "This notebook is going to vectorize a speech by Dr A.P.J. Abdul Kalam using scikit-learn's implementation of BoWs - CountVectorizer.\n",
    "\n",
    "* We are going to implement a custom preprocessor, to normalize the corpus before applying vectorization.\n",
    "* Implement vectorization using sci-kit learn's CountVectorizer.\n",
    "* Visualize the vectorized form using a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1ffc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e2b82af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sharo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sharo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sharo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sharo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # To define english stopwords\n",
    "nltk.download('punkt') # For sentence tokenizer to work\n",
    "nltk.download('averaged_perceptron_tagger') # Contains Point of Speech Tags\n",
    "nltk.download('wordnet') # For lemmatizer to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1559702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488e035",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02fd96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam - to be vectorized\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4ccfb",
   "metadata": {},
   "source": [
    "# Creating a custom preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "385f6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \n",
    "    \"\"\"The Lemmatizer function in nltk takes a 'Part of Speech'(pos) variable as argument, \n",
    "    which denotes the pos of the word in the language. This function finds out the pos of the\n",
    "    word using the WordNet database and simplifies it into one of the 4 pos_tags that nltk \n",
    "    lemmatizer allows - [n, v, a, r, s]\"\"\"\n",
    "    \n",
    "    pos = pos_tag([word])[0][1]\n",
    "    \n",
    "    if pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "233b8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "\n",
    "def custom_preprocessor(sentence):\n",
    "    \"\"\"\n",
    "    Function takes in an English sentence and returns a lower-case, lemmatized version of the same, without stopwords.\n",
    "    \"\"\"\n",
    "    new_sentence = []\n",
    "    \n",
    "    words = word_tokenize(sentence) # words now contains list of words in the sentence\n",
    "    \n",
    "    # Creating a list of lower-case, lemmatized words, while filtering out stopwords\n",
    "    words = [lm.lemmatize(word.lower(), pos=get_wordnet_pos(word)) for word in words if word.lower() not in eng_stopwords]\n",
    "    \n",
    "    new_sentence = ' '.join(words)\n",
    "            \n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a517c",
   "metadata": {},
   "source": [
    "# Creating BoW using ContVectorizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ff0d343",
   "metadata": {},
   "source": [
    "CountVectorizer(\n",
    "    *,\n",
    "    input='content',\n",
    "    encoding='utf-8',\n",
    "    decode_error='strict',\n",
    "    strip_accents=None,\n",
    "    lowercase=True,\n",
    "    preprocessor=None,\n",
    "    tokenizer=None,\n",
    "    stop_words=None,\n",
    "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=1.0,\n",
    "    min_df=1,\n",
    "    max_features=None,\n",
    "    vocabulary=None,\n",
    "    binary=False,\n",
    "    dtype=<class 'numpy.int64'>,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f1dabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing CountVectorizer\n",
    "cv = CountVectorizer(input='content', \n",
    "                     preprocessor=custom_preprocessor, # Sends each sentence to our custom preprocessor\n",
    "                     tokenizer=word_tokenize, # Users word_tokenizer module from nltk\n",
    "                     token_pattern=None # To hide warning, which would otherwise show when tokenizer is not None\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2d88fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our paragraph into a list of sentences, because CountVectorizer expects \n",
    "# sequence of items that can be of type string, when input='content'\n",
    "sentences = sent_tokenize(text=paragraph, language='english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f957ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cv.fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "957dbda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 121)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fcd991fe",
   "metadata": {},
   "source": [
    "31 sentences and 121 unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c4c2f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', '.', '10', '1857', '3000', '5', '?', 'achievement',\n",
       "       'alexander', 'also', 'among', 'anyone', 'area', 'believe', 'brahm',\n",
       "       'british', 'build', 'capture', 'career', 'closely', 'come',\n",
       "       'conquer', 'consider', 'culture', 'dept', 'develop', 'developed',\n",
       "       'development', 'dhawan', 'do', 'dr.', 'dutch', 'economic',\n",
       "       'enforce', 'fall', 'father', 'fifty', 'first', 'fortune', 'four',\n",
       "       'free', 'freedom', 'french', 'gdp', 'get', 'globally', 'go',\n",
       "       'good', 'grabbed', 'great', 'greek', 'growth', 'hand-in-hand',\n",
       "       'history', 'incorrect', 'independence', 'india', 'invade', 'lack',\n",
       "       'land', 'level', 'life', 'loot', 'lucky', 'material', 'milestone',\n",
       "       'military', 'mind', 'mogul', 'must', 'nation', 'nuclear',\n",
       "       'nurture', 'one', 'onwards', 'opportunity', 'others.that',\n",
       "       'people', 'percent', 'portuguese', 'poverty', 'power', 'prakash',\n",
       "       'professor', 'protect', 'rate', 'recognise', 'respect', 'sarabhai',\n",
       "       'satish', 'second', 'see', 'self-assured', 'self-confidence',\n",
       "       'self-reliant', 'space', 'stand', 'start', 'strength', 'strong',\n",
       "       'succeed', 'take', 'term', 'third', 'three', 'time', 'today',\n",
       "       'top', 'try', 'turk', 'u', 'unless', 'vikram', 'vision', 'war',\n",
       "       'way', 'work', 'world', 'year', 'yet', '’'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique words\n",
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0413e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique words\n",
    "len(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8a9b3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'three': 104,\n",
       " 'vision': 113,\n",
       " 'india': 56,\n",
       " '.': 1,\n",
       " '3000': 4,\n",
       " 'year': 118,\n",
       " 'history': 53,\n",
       " ',': 0,\n",
       " 'people': 77,\n",
       " 'world': 117,\n",
       " 'come': 20,\n",
       " 'invade': 57,\n",
       " 'u': 110,\n",
       " 'capture': 17,\n",
       " 'land': 59,\n",
       " 'conquer': 21,\n",
       " 'mind': 67,\n",
       " 'alexander': 8,\n",
       " 'onwards': 74,\n",
       " 'greek': 50,\n",
       " 'turk': 109,\n",
       " 'mogul': 68,\n",
       " 'portuguese': 79,\n",
       " 'british': 15,\n",
       " 'french': 42,\n",
       " 'dutch': 31,\n",
       " 'loot': 62,\n",
       " 'take': 101,\n",
       " 'yet': 119,\n",
       " 'do': 29,\n",
       " 'nation': 70,\n",
       " 'anyone': 11,\n",
       " 'grabbed': 48,\n",
       " 'culture': 23,\n",
       " 'try': 108,\n",
       " 'enforce': 33,\n",
       " 'way': 115,\n",
       " 'life': 61,\n",
       " '?': 6,\n",
       " 'respect': 87,\n",
       " 'freedom': 41,\n",
       " 'others.that': 76,\n",
       " 'first': 37,\n",
       " 'believe': 13,\n",
       " 'get': 44,\n",
       " '1857': 3,\n",
       " 'start': 97,\n",
       " 'war': 114,\n",
       " 'independence': 55,\n",
       " 'must': 69,\n",
       " 'protect': 84,\n",
       " 'nurture': 72,\n",
       " 'build': 16,\n",
       " 'free': 40,\n",
       " 'one': 73,\n",
       " 'second': 90,\n",
       " '’': 120,\n",
       " 'development': 27,\n",
       " 'fifty': 36,\n",
       " 'develop': 25,\n",
       " 'time': 105,\n",
       " 'see': 91,\n",
       " 'developed': 26,\n",
       " 'among': 10,\n",
       " 'top': 107,\n",
       " '5': 5,\n",
       " 'term': 102,\n",
       " 'gdp': 43,\n",
       " '10': 2,\n",
       " 'percent': 78,\n",
       " 'growth': 51,\n",
       " 'rate': 85,\n",
       " 'area': 12,\n",
       " 'poverty': 80,\n",
       " 'level': 60,\n",
       " 'fall': 34,\n",
       " 'achievement': 7,\n",
       " 'globally': 45,\n",
       " 'recognise': 86,\n",
       " 'today': 106,\n",
       " 'lack': 58,\n",
       " 'self-confidence': 93,\n",
       " 'self-reliant': 94,\n",
       " 'self-assured': 92,\n",
       " 'incorrect': 54,\n",
       " 'third': 103,\n",
       " 'stand': 96,\n",
       " 'unless': 111,\n",
       " 'strength': 98,\n",
       " 'strong': 99,\n",
       " 'military': 66,\n",
       " 'power': 81,\n",
       " 'also': 9,\n",
       " 'economic': 32,\n",
       " 'go': 46,\n",
       " 'hand-in-hand': 52,\n",
       " 'good': 47,\n",
       " 'fortune': 38,\n",
       " 'work': 116,\n",
       " 'great': 49,\n",
       " 'dr.': 30,\n",
       " 'vikram': 112,\n",
       " 'sarabhai': 88,\n",
       " 'dept': 24,\n",
       " 'space': 95,\n",
       " 'professor': 83,\n",
       " 'satish': 89,\n",
       " 'dhawan': 28,\n",
       " 'succeed': 100,\n",
       " 'brahm': 14,\n",
       " 'prakash': 82,\n",
       " 'father': 35,\n",
       " 'nuclear': 71,\n",
       " 'material': 64,\n",
       " 'lucky': 63,\n",
       " 'closely': 19,\n",
       " 'consider': 22,\n",
       " 'opportunity': 75,\n",
       " 'four': 39,\n",
       " 'milestone': 65,\n",
       " 'career': 18}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Unique Word, Column Number)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd997c5c",
   "metadata": {},
   "source": [
    "# Vectorized Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad55fc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [3, 1, 0, ..., 1, 0, 0],\n",
       "       [9, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [3, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3979f",
   "metadata": {},
   "source": [
    "# Converting to dataframe for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57ce6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = pd.DataFrame(data=x, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cecfffa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>10</th>\n",
       "      <th>1857</th>\n",
       "      <th>3000</th>\n",
       "      <th>5</th>\n",
       "      <th>?</th>\n",
       "      <th>achievement</th>\n",
       "      <th>alexander</th>\n",
       "      <th>also</th>\n",
       "      <th>...</th>\n",
       "      <th>unless</th>\n",
       "      <th>vikram</th>\n",
       "      <th>vision</th>\n",
       "      <th>war</th>\n",
       "      <th>way</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>’</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ,  .  10  1857  3000  5  ?  achievement  alexander  also  ...  unless  \\\n",
       "0  0  1   0     0     0  0  0            0          0     0  ...       0   \n",
       "1  3  1   0     0     1  0  0            0          0     0  ...       0   \n",
       "2  9  1   0     0     0  0  0            0          1     0  ...       0   \n",
       "3  0  1   0     0     0  0  0            0          0     0  ...       0   \n",
       "4  0  1   0     0     0  0  0            0          0     0  ...       0   \n",
       "\n",
       "   vikram  vision  war  way  work  world  year  yet  ’  \n",
       "0       0       1    0    0     0      0     0    0  0  \n",
       "1       0       0    0    0     0      1     1    0  0  \n",
       "2       0       0    0    0     0      0     0    0  0  \n",
       "3       0       0    0    0     0      0     0    1  0  \n",
       "4       0       0    0    0     0      0     0    0  0  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4567ec02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>3000</th>\n",
       "      <th>capture</th>\n",
       "      <th>come</th>\n",
       "      <th>conquer</th>\n",
       "      <th>history</th>\n",
       "      <th>invade</th>\n",
       "      <th>land</th>\n",
       "      <th>mind</th>\n",
       "      <th>people</th>\n",
       "      <th>u</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ,    .  3000  capture  come  conquer  history  invade  land  mind  \\\n",
       "0  3.0  1.0   1.0      1.0   1.0      1.0      1.0     1.0   1.0   1.0   \n",
       "\n",
       "   people    u  world  year  \n",
       "0     1.0  1.0    1.0   1.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe out of the vector representation of the ith sentence, filtering out just the terms that occur in that \n",
    "# sentence\n",
    "\n",
    "i = 2 # You can set any i, where i is the sentence number\n",
    "\n",
    "ith_sentence = vector[vector > 0].iloc[i-1, :].dropna() # Pandas Series\n",
    "\n",
    "ith_sentence = pd.DataFrame(data= [ith_sentence.values], columns=ith_sentence.index) # Pandas DataFrame\n",
    "\n",
    "ith_sentence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6279f8a8",
   "metadata": {},
   "source": [
    "Second sentence:\n",
    "\n",
    "\"\"\"\n",
    "In 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds.\n",
    "\"\"\"          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f9200",
   "metadata": {},
   "source": [
    "We can see how the second sentence has been vectorized, with column names as terms and values set to the frequency of occurence of each term. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
